{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (auc, classification_report, roc_auc_score, accuracy_score,\n",
    "                             f1_score, log_loss, roc_curve, confusion_matrix,\n",
    "                             precision_score, recall_score, plot_confusion_matrix,\n",
    "                             make_scorer)\n",
    "\n",
    "import xgboost as xgb\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign Model\n",
    "model_t = XGBClassifier(n_estimators= 70, max_depth=4, eta = .05, subsample = .9, colsample_bytree = .8)\n",
    "\n",
    "# #Utilize Cross Validation To Further Enhance Model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=8, random_state=1)\n",
    "scores = cross_val_score(model_t, X_train, y_train, cv=cv, scoring = 'neg_log_loss', n_jobs = -1, error_score = 'raise')\n",
    "\n",
    "scores = abs(scores)\n",
    "print('Mean Log Loss: %.3f (%.3f)' % (scores.mean(), scores.std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build The Model\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", \n",
    "                              random_state = 42, \n",
    "                              eta = .04, \n",
    "                              max_depth = 6,\n",
    "                              min_child_weight = 3,\n",
    "                              n_estimators = 100,\n",
    "                              gamma = .6,\n",
    "                              reg_lambda = .2,\n",
    "                              subsample = 1,\n",
    "                              colsample_bytree = .99)\n",
    "\n",
    "#Fit The Model\n",
    "xgb_model.fit(X, Y, early_stopping_rounds = 5, eval_metric = 'logloss', eval_set = [(x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update The Model Based On Cross Validation Tests and Fit and Balance Overfitting With Performance\n",
    "model_t = XGBClassifier(n_estimators= 40, max_depth = 4, eta = .05, subsample = .8, colsample_bytree = .9)\n",
    "print(\"                 Test Set                        Train Set\")\n",
    "model_t.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)]) #Check For Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "param_grid = {'eta': [.0035],\n",
    "              'objective':['binary:logistic'],\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [1],\n",
    "              'n_estimators': [8],\n",
    "              'gamma': [.44],\n",
    "              'reg_lambda' : [.55],\n",
    "              'subsample': [1],\n",
    "              'colsample_bytree': [.5]}\n",
    "\n",
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "optimal_params = GridSearchCV(xgb_model, param_grid, n_jobs = 4, scoring = LogLoss, verbose = 0, cv = 3)\n",
    "\n",
    "optimal_params.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score\n",
    "accuracy_score(test['team1_win'], test['prediction'])\n",
    "precision_score(test['team1_win'], test['prediction'])\n",
    "recall_score(test['team1_win'], test['prediction'])\n",
    "f1_score(test['team1_win'], test['prediction'])\n",
    "log_loss(test['team1_win'].values, test['prob'].values, labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(xgb_model.feature_importances_, index= X.columns)\n",
    "feat_importances.nlargest(54).plot(kind='barh')\n",
    "sns.set(rc = {'figure.figsize':(14,14)})\n",
    "plt.title(\"Feature Importance of XGBoost Model\", size = 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
